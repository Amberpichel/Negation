{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_EMC.tsv', 'r') as infile, open('no_duplicates.tsv', 'w') as outfile: \n",
    "    seen = set()\n",
    "    for line in infile:\n",
    "        if line in seen:\n",
    "            continue  \n",
    "        seen.add(line)\n",
    "        outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('no_duplicates.tsv', delimiter = '\\t')\n",
    "msk = np.random.rand(len(df)) <= 0.8\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "train.to_csv('train.tsv', sep = '\\t', index = False)\n",
    "test.to_csv('test.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(inputfile):\n",
    "    \"\"\"Extract features from the test data\"\"\"   \n",
    "    data = []\n",
    "    with open(inputfile, 'r', encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            components = line.rstrip('\\n').split()\n",
    "            if len(components) > 0:\n",
    "                offsetword = components[0]\n",
    "                prev_tokens_one = components[1]\n",
    "                prev_tokens_two = components[2]\n",
    "                prev_tokens_three = components[3]\n",
    "                prev_tokens_four = components[4]\n",
    "                next_tokens_one = components[5]\n",
    "                next_tokens_two = components[6]\n",
    "                next_tokens_three = components[7]\n",
    "                pos_prev_one = components[7]\n",
    "                pos_prev_two = components[8]\n",
    "                pos_prev_three = components[9]\n",
    "                pos_prev_four = components[10]\n",
    "                pos_next_one = components[11]\n",
    "                pos_next_two = components[12]\n",
    "                pos_next_three = components[13]\n",
    "                neg_prev_one = components[14]\n",
    "                neg_prev_two = components[15]\n",
    "                neg_prev_three = components[16]\n",
    "                neg_prev_four = components[17]\n",
    "                neg_next_one = components[18]\n",
    "                neg_next_two = components[19]\n",
    "                neg_next_three = components[20]\n",
    "                neg = components[21]\n",
    "                \n",
    "                feature_dict = {'Event': offsetword,\n",
    "                                'Previous token one': prev_tokens_one[0],\n",
    "                                'Previous token two': prev_tokens_two[0],\n",
    "                                'Previous token three': prev_tokens_three[0],\n",
    "                                'Previous token four': prev_tokens_four[0],\n",
    "                                'Next token one': next_tokens_one[0],\n",
    "                                'Next token two': next_tokens_two[0],\n",
    "                                'Next token three': next_tokens_three[0],\n",
    "                                'POS previous token one': pos_prev_one[0],\n",
    "                                'POS previous token two': pos_prev_two[0],\n",
    "                                'POS previous token three': pos_prev_three[0],\n",
    "                                'POS previous token four': pos_prev_four[0],\n",
    "                                'POS next token one': pos_next_one[0],\n",
    "                                'POS next token two': pos_next_two[0],\n",
    "                                'POS next token three': pos_next_three[0],\n",
    "                                'Negcue prev token one': neg_prev_one[0],\n",
    "                                'Negcue prev token two': neg_prev_two[0],\n",
    "                                'Negcue prev token three': neg_prev_three[0],\n",
    "                                'Negcue prev token four': neg_prev_four[0],\n",
    "                                'Negcue next token one': neg_next_one[0],\n",
    "                                'Negcue next token two': neg_next_two[0],\n",
    "                                'Negcue next token three': neg_next_three[0],\n",
    "                                'Negation': neg}\n",
    "                data.append(feature_dict)\n",
    "                \n",
    "    return data, targets\n",
    "\n",
    "    outfile = open(outputfile, 'w')\n",
    "    counter = 0\n",
    "    for line in open(inputdata, 'r'):\n",
    "        if len(line.rstrip('\\n').split()) > 0:\n",
    "            outfile.write(line.rstrip('\\n') + '\\t' + predictions[counter] + '\\n')\n",
    "            counter += 1\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('half_notes_two.tsv', delimiter = '\\t')\n",
    "msk = np.random.rand(len(df)) <= 0.5\n",
    "\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "\n",
    "train.to_csv('half_notes_five.tsv', sep = '\\t', index = False)\n",
    "test.to_csv('half_notes_six.tsv', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

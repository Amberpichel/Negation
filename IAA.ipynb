{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edwin amber\n",
      "Percentage Agreement: 0.99\n",
      "Cohen's Kappa: 0.82\n",
      "            B-NegCue  I-NegCue  B-NegEvent  I-NegEvent     _\n",
      "B-NegCue          30         0           0           0     3\n",
      "I-NegCue           0         2           0           0     1\n",
      "B-NegEvent         0         0          28           1     5\n",
      "I-NegEvent         0         0           1           4     3\n",
      "_                  6         0           8           0  2980\n",
      "amber dilara\n",
      "Percentage Agreement: 0.99\n",
      "Cohen's Kappa: 0.90\n",
      "            B-NegCue  I-NegCue  B-NegEvent  I-NegEvent     _\n",
      "B-NegCue          36         0           0           0     1\n",
      "I-NegCue           0         2           0           0     0\n",
      "B-NegEvent         0         0          34           0     3\n",
      "I-NegEvent         0         0           1           3     1\n",
      "_                  3         0           6           1  2982\n",
      "edwin dilara\n",
      "Percentage Agreement: 0.99\n",
      "Cohen's Kappa: 0.82\n",
      "            B-NegCue  I-NegCue  B-NegEvent  I-NegEvent     _\n",
      "B-NegCue          32         0           0           0     2\n",
      "I-NegCue           0         2           0           0     2\n",
      "B-NegEvent         0         0          31           0     4\n",
      "I-NegEvent         0         0           1           3     4\n",
      "_                  7         0           9           1  2977\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os.path\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "categories = [\"B-NegCue\", \"I-NegCue\", \"B-NegEvent\", \"I-NegEvent\", \"_\"]\n",
    "\n",
    "''''Code from Language as Data lab 3.2 Evaluate Annotations'''\n",
    "\n",
    "def collect_files_1():\n",
    "    '''\n",
    "        \tCollect all the annotation files in the specified directory\n",
    "        \tand put them in a dictionary with each annotator and their annotations.\n",
    "        \t:param dir: path to the directory the annotationsheets are placed in\n",
    "        \t:type dir: string\n",
    "        \t:returns: dictionary with annotations per annotator\n",
    "        \t'''\n",
    "    annotations = {}\n",
    "    for sheet in glob.glob(\"Anno_1/*.tsv\"):\n",
    "        filename, extension = os.path.basename(sheet).split(\".\")\n",
    "        prefix, annotator= filename.split(\"_\")\n",
    "\n",
    "        # Read in annotations\n",
    "        annotation_data = pd.read_csv(sheet, sep=\"\\t\", header=0, keep_default_na=False)\n",
    "        annotations[annotator] = annotation_data[\"Annotation\"]\n",
    "\n",
    "    for annotator_a, annotator_b in combinations(annotations.keys(), len(annotations.keys())):\n",
    "        # calculate the agreement percentage\n",
    "        agreement = [anno1 == anno2 for anno1, anno2 in  zip(annotations[annotator_a], annotations[annotator_b])]\n",
    "        percentage = sum(agreement)/len(agreement)\n",
    "        print(annotator_a, annotator_b, )\n",
    "        print(\"Percentage Agreement: %.2f\" %percentage)\n",
    "        #calculate cohen's kappa\n",
    "        kappa = cohen_kappa_score(annotations[annotator_a], annotations[annotator_b], labels=categories)\n",
    "        print(\"Cohen's Kappa: %.2f\" %kappa)\n",
    "        #provide the confusion matrix\n",
    "        confusions = confusion_matrix(annotations[annotator_a], annotations[annotator_b], labels=categories)\n",
    "        #print(confusions)\n",
    "        matrix= pd.DataFrame(confusions, index=categories, columns=categories)\n",
    "        print(matrix)\n",
    "        #write results to a txt\n",
    "        #with open(f'{dir}/annotation_evaluation.txt', 'w', encoding='utf8') as outfile:\n",
    "            #outfile.write(\"Percentage Agreement: %.2f\\n\" %percentage)\n",
    "            #outfile.write(\"Cohen's Kappa: %.2f\\n\" %kappa)\n",
    "            #outfile.write(matrix.to_markdown())\n",
    "\n",
    "collect_files_1()\n",
    "\n",
    "def collect_files_2():\n",
    "    '''\n",
    "        \tCollect all the annotation files in the specified directory\n",
    "        \tand put them in a dictionary with each annotator and their annotations.\n",
    "        \t:param dir: path to the directory the annotationsheets are placed in\n",
    "        \t:type dir: string\n",
    "        \t:returns: dictionary with annotations per annotator\n",
    "        \t'''\n",
    "    annotations = {}\n",
    "    for sheet in glob.glob(\"Anno_2/*.tsv\"):\n",
    "        filename, extension = os.path.basename(sheet).split(\".\")\n",
    "        prefix, annotator= filename.split(\"_\")\n",
    "\n",
    "        # Read in annotations\n",
    "        annotation_data = pd.read_csv(sheet, sep=\"\\t\", header=0, keep_default_na=False)\n",
    "        annotations[annotator] = annotation_data[\"Annotation\"]\n",
    "\n",
    "    for annotator_a, annotator_b in combinations(annotations.keys(), len(annotations.keys())):\n",
    "        # calculate the agreement percentage\n",
    "        agreement = [anno1 == anno2 for anno1, anno2 in  zip(annotations[annotator_a], annotations[annotator_b])]\n",
    "        percentage = sum(agreement)/len(agreement)\n",
    "        print(annotator_a, annotator_b, )\n",
    "        print(\"Percentage Agreement: %.2f\" %percentage)\n",
    "        #calculate cohen's kappa\n",
    "        kappa = cohen_kappa_score(annotations[annotator_a], annotations[annotator_b], labels=categories)\n",
    "        print(\"Cohen's Kappa: %.2f\" %kappa)\n",
    "        #provide the confusion matrix\n",
    "        confusions = confusion_matrix(annotations[annotator_a], annotations[annotator_b], labels=categories)\n",
    "        #print(confusions)\n",
    "        matrix= pd.DataFrame(confusions, index=categories, columns=categories)\n",
    "        print(matrix)\n",
    "        #write results to a txt\n",
    "        #with open(f'{dir}/annotation_evaluation.txt', 'w', encoding='utf8') as outfile:\n",
    "            #outfile.write(\"Percentage Agreement: %.2f\\n\" %percentage)\n",
    "            #outfile.write(\"Cohen's Kappa: %.2f\\n\" %kappa)\n",
    "            #outfile.write(matrix.to_markdown())\n",
    "\n",
    "collect_files_2()\n",
    "\n",
    "def collect_files_3():\n",
    "    '''\n",
    "        \tCollect all the annotation files in the specified directory\n",
    "        \tand put them in a dictionary with each annotator and their annotations.\n",
    "        \t:param dir: path to the directory the annotationsheets are placed in\n",
    "        \t:type dir: string\n",
    "        \t:returns: dictionary with annotations per annotator\n",
    "        \t'''\n",
    "    annotations = {}\n",
    "    for sheet in glob.glob(\"Anno_3/*.tsv\"):\n",
    "        filename, extension = os.path.basename(sheet).split(\".\")\n",
    "        prefix, annotator= filename.split(\"_\")\n",
    "\n",
    "        # Read in annotations\n",
    "        annotation_data = pd.read_csv(sheet, sep=\"\\t\", header=0, keep_default_na=False)\n",
    "        annotations[annotator] = annotation_data[\"Annotation\"]\n",
    "\n",
    "    for annotator_a, annotator_b in combinations(annotations.keys(), len(annotations.keys())):\n",
    "        # calculate the agreement percentage\n",
    "        agreement = [anno1 == anno2 for anno1, anno2 in  zip(annotations[annotator_a], annotations[annotator_b])]\n",
    "        percentage = sum(agreement)/len(agreement)\n",
    "        print(annotator_a, annotator_b, )\n",
    "        print(\"Percentage Agreement: %.2f\" %percentage)\n",
    "        #calculate cohen's kappa\n",
    "        kappa = cohen_kappa_score(annotations[annotator_a], annotations[annotator_b], labels=categories)\n",
    "        print(\"Cohen's Kappa: %.2f\" %kappa)\n",
    "        #provide the confusion matrix\n",
    "        confusions = confusion_matrix(annotations[annotator_a], annotations[annotator_b], labels=categories)\n",
    "        #print(confusions)\n",
    "        matrix= pd.DataFrame(confusions, index=categories, columns=categories)\n",
    "        print(matrix)\n",
    "        #write results to a txt\n",
    "        #with open(f'{dir}/annotation_evaluation.txt', 'w', encoding='utf8') as outfile:\n",
    "            #outfile.write(\"Percentage Agreement: %.2f\\n\" %percentage)\n",
    "            #outfile.write(\"Cohen's Kappa: %.2f\\n\" %kappa)\n",
    "            #outfile.write(matrix.to_markdown())\n",
    "\n",
    "collect_files_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
